{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RqOrDrMYEn7pqU09VhnWDVClb0kXsVF9",
      "authorship_tag": "ABX9TyPutFofaE3WI+a0Ay2t6HjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/MidiTurmoil/blob/main/MIDI_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">MIDI Scraper <font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "Scrapes midi files from mididb.com (artist URL, genre URL), freemidi.org (artist URL) or any web page with links to midi files.\n",
        "\n",
        "`output_dir` should be relative to your Google Drive root, e.g. `music/midis` if you have a directory called *music* in your Drive, containing a subdirectory called *midis*, and that's the directory you want to save all scraped midi files.\n",
        "\n",
        "In mididb.com or freemidi.org scrapes, a subdirectory for each artist is auto-created under `output_dir` (e.g. `music/midis/aphex-twin`). In other scrapes, the subdirectory name will be website's domain name."
      ],
      "metadata": {
        "id": "elb0G9bVHZK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "\n",
        "force_setup = False\n",
        "repositories = []\n",
        "pip_packages = ''\n",
        "apt_packages = ''\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "skip_setup = False #@ param {type:\"boolean\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  if apt_packages != '':\n",
        "    !apt-get update && apt-get install {apt_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive is True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if len(repositories) > 0 and skip_setup == False:\n",
        "  for repo in repositories:\n",
        "    %cd /content/\n",
        "    install_dir = fix_path('/content/'+path_leaf(repo).replace('.git', ''))\n",
        "    repo = repo if '.git' in repo else repo+'.git'\n",
        "    !git clone {repo}\n",
        "    if os.path.isfile(install_dir+'setup.py') or os.path.isfile(install_dir+'setup.cfg'):\n",
        "      !pip install -e ./{install_dir}\n",
        "    if os.path.isfile(install_dir+'requirements.txt'):\n",
        "      !pip install -r {install_dir}/requirements.txt\n",
        "\n",
        "if len(repositories) == 1:\n",
        "  %cd {install_dir}\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "create_dirs([dir_tmp])\n",
        "\n",
        "import time, sys\n",
        "from datetime import timedelta\n",
        "\n",
        "import urllib.request\n",
        "import requests\n",
        "\n",
        "\n",
        "try: \n",
        "  from BeautifulSoup import BeautifulSoup\n",
        "except ImportError:\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "def wild_scrape(page_url, out_dir):\n",
        "  global got_em, tot_em, total_input_urls\n",
        "\n",
        "  http = page_url.split('/')\n",
        "  host = http[2]\n",
        "  base_url = http[0]+'//'+host+'/'\n",
        "  base_www = http[0]+'//www.'+host+'/'\n",
        "\n",
        "  headers = {\n",
        "    \"Host\": host,\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "  }\n",
        "\n",
        "  page = urllib.request.urlopen(page_url)\n",
        "  html = BeautifulSoup(page.read())\n",
        "  links = nav = html.body.find_all('a', href=True)\n",
        "  tracks = []\n",
        "\n",
        "  for link in links:\n",
        "    href = str(link['href'])\n",
        "    if href[:4] != 'http':\n",
        "      href = page_url.replace(path_leaf(page_url), '')+href\n",
        "    test1 = href[-4:]\n",
        "    test2 = href[-5:]\n",
        "    if test1.lower() == '.mid' or test2.lower() == '.midi':\n",
        "      filename = path_leaf(href)\n",
        "      tracks.append([filename, href])\n",
        "\n",
        "  total_tracks = len(tracks)\n",
        "  for iii, track in enumerate(tracks, 1):\n",
        "    iii_ndx_info = str(iii)+'/'+str(total_tracks)+' '\n",
        "    link = str(track[1])\n",
        "    dir_out = out_dir+fix_path(host.replace('www.', ''))\n",
        "    if not os.path.isdir(dir_out):\n",
        "      os.mkdir(dir_out)\n",
        "    filename = dir_out+str(track[0])\n",
        "\n",
        "    if os.path.isfile(filename):\n",
        "      op(c.warn, iii_ndx_info+'File already exists, skipping ', filename.replace(drive_root, ''), time=True)\n",
        "      tot_em = tot_em+1\n",
        "    else:\n",
        "      faux_req = session.get(link, headers=headers)\n",
        "      req = session.get(link, headers=headers)\n",
        "      with open(filename, \"wb\") as mid:\n",
        "        mid.write(req.content)\n",
        "      if os.path.isfile(filename):\n",
        "        op(c.ok, iii_ndx_info+'Saved', filename.replace(drive_root, ''), time=True)\n",
        "        got_em = got_em+1\n",
        "        tot_em = tot_em+1\n",
        "      else:\n",
        "        op(c.fail, iii_ndx_info+'ERROR saving', link, time=True)\n",
        "        tot_em = tot_em+1\n",
        "\n",
        "def scrape(page_url, out_dir):\n",
        "  global got_em, tot_em, total_input_urls\n",
        "\n",
        "  if 'mididb.com' in page_url:\n",
        "    host = 'mididb.com'\n",
        "    scraping = 'mididb'\n",
        "  elif 'freemidi.org' in page_url:\n",
        "    host = 'freemidi.org'\n",
        "    scraping = 'freemidi'\n",
        "  else:\n",
        "    return 'URL not supported.'\n",
        "\n",
        "  base_url = 'https://'+host+'/'\n",
        "  base_www = 'https://www.'+host+'/'\n",
        "  if scraping == 'mididb' and page_url[-1] != '/':\n",
        "    page_url = page_url+'/'\n",
        "\n",
        "  headers = {\n",
        "    \"Host\": host,\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "  }\n",
        "\n",
        "  page = urllib.request.urlopen(page_url)\n",
        "  html = BeautifulSoup(page.read())\n",
        "  page_inputs = []\n",
        "\n",
        "  if scraping == 'freemidi':\n",
        "    nav = html.body.find('div', attrs={'class', 'artist-container'}).find('nav').find('ul').find_all('li')\n",
        "    page_inputs.append(page_url)\n",
        "    for n in nav:\n",
        "      href = str(n.find('a')['href'])\n",
        "      if href != '#': \n",
        "        page_inputs.append(str(base_url+href))\n",
        "\n",
        "  if scraping == 'mididb':\n",
        "    if html.body.find('select', attrs={'class', 'jumpmenu'}):\n",
        "      nav = html.body.find('select', attrs={'class', 'jumpmenu'}).find_all('option')\n",
        "      for n in nav:\n",
        "        tail = str(n.attrs['value'])\n",
        "        tail = tail.replace('genres.asp?permalink=/', '')\n",
        "        if tail[0] == '/':\n",
        "          tail = tail[1:]\n",
        "        page_inputs.append(base_www+tail)\n",
        "    else:\n",
        "      page_inputs = [page_url]\n",
        "\n",
        "  total_pages = len(page_inputs)\n",
        "\n",
        "  for ii, input in enumerate(page_inputs, 1):\n",
        "    tracks = []\n",
        "    ii_ndx_info = str(ii)+'/'+str(total_pages)+' '\n",
        "    op(c.title, ii_ndx_info+'Fetching from page', input, time=True)\n",
        "\n",
        "    # faux_req = session.get(input, headers=headers)\n",
        "    # page2 = session.get(input, headers=headers)\n",
        "    # html2 = BeautifulSoup(page2)\n",
        "    page = None\n",
        "    html = None\n",
        "    page = urllib.request.urlopen(input)\n",
        "    html = BeautifulSoup(page.read())\n",
        "    \n",
        "    if scraping == 'mididb':\n",
        "      cnts = html.body.find_all('span', attrs={'class', 'song-title'})\n",
        "      for cnt in cnts:\n",
        "        href = cnt.find('a').attrs['href']\n",
        "        midi_link = base_url+'midi-download/'+cnt.attrs['id'].replace('s_', 'AUD_')+'.mid'\n",
        "        artist = href.split('/')[3]\n",
        "        filename = href.split('/')[4]\n",
        "        tracks.append([artist, filename, midi_link])\n",
        "\n",
        "    if scraping == 'freemidi':\n",
        "      cnts = html2.body.find_all('span', attrs={'itemprop':'name'})\n",
        "      for cnt in cnts:\n",
        "        href = cnt.find('a').attrs['href']\n",
        "        pcs = href.split('-')\n",
        "        id = pcs[1]\n",
        "        filename = '-'.join(pcs[2:])\n",
        "        midi_link = base_url+'getter-'+id\n",
        "        artist = '-'.join(page_url.split('-')[2:])\n",
        "        tracks.append([artist, filename, midi_link])\n",
        "    \n",
        "    total_tracks = len(tracks)\n",
        "    for iii, track in enumerate(tracks, 1):\n",
        "      iii_ndx_info = str(iii)+'/'+str(total_tracks)+' '\n",
        "      artist = str(track[0])\n",
        "      title = str(track[1])\n",
        "      link = str(track[2])\n",
        "      dir_out = out_dir+fix_path(artist)\n",
        "      if not os.path.isdir(dir_out):\n",
        "        os.mkdir(dir_out)\n",
        "      filename = dir_out+title+'.mid'\n",
        "\n",
        "      if os.path.isfile(filename):\n",
        "        op(c.warn, iii_ndx_info+'File already exists, skipping ', filename.replace(drive_root, ''), time=True)\n",
        "        tot_em = tot_em+1\n",
        "      else:\n",
        "        faux_req = session.get(link, headers=headers)\n",
        "        req = session.get(link, headers=headers)\n",
        "        with open(filename, \"wb\") as mid:\n",
        "          mid.write(req.content)\n",
        "        if os.path.isfile(filename):\n",
        "          op(c.ok, iii_ndx_info+'Saved', filename.replace(drive_root, ''), time=True)\n",
        "          got_em = got_em+1\n",
        "          tot_em = tot_em+1\n",
        "        else:\n",
        "          op(c.fail, iii_ndx_info+'ERROR saving', link, time=True)\n",
        "          tot_em = tot_em+1\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D0MnqifiHdBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3f65b1-4e6b-4c20-9fd7-57de442efca2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mSetup finished.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Do stuff\n",
        "# You may enter multiple URLs, use ; as separator.\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "output_dir = \"\" #@param {type:\"string\"}\n",
        "end_session_when_done = False #@ param {type: \"boolean\"}\n",
        "\n",
        "uniq_id = gen_id()\n",
        "\n",
        "# You may enter URLs to this list variable, enter 'list' to url: field to use it:\n",
        "url_list = []\n",
        "\n",
        "\n",
        "if url == 'list':\n",
        "  urls = url_list\n",
        "else:\n",
        "  if ';' in url:\n",
        "    urls = [x.strip() for x in url.split(';')]\n",
        "  else:\n",
        "    urls = [url]\n",
        "\n",
        "inputs = urls\n",
        "\n",
        "# Output\n",
        "if output_dir != '':\n",
        "  if not os.path.isdir(drive_root+output_dir):\n",
        "    os.mkdir(drive_root+output_dir)\n",
        "  out_dir = drive_root+fix_path(output_dir)\n",
        "  \n",
        "timer_start = time.time()\n",
        "total = len(inputs)\n",
        "\n",
        "got_em = 0\n",
        "tot_em = 0\n",
        "\n",
        "# -- DO THINGS --\n",
        "for i, page_url in enumerate(inputs, 1):\n",
        "  ndx_info = str(i)+'/'+str(total)+' '\n",
        "  print()\n",
        "  op(c.title, ndx_info+'Scraping URL', page_url, time=True)\n",
        "  print()\n",
        "  if 'freemidi.org' in page_url or 'mididb.com' in page_url:\n",
        "    scrape(page_url, out_dir)\n",
        "  else:\n",
        "    wild_scrape(page_url, out_dir)\n",
        "# -- END THINGS --\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "timer_end = time.time()\n",
        "\n",
        "op(c.ok, 'Saved '+str(got_em)+'/'+str(tot_em)+' files.', time=True)\n",
        "op(c.okb, 'Elapsed', timedelta(seconds=timer_end-timer_start), time=True)\n",
        "op(c.ok, 'FIN.')\n",
        "\n",
        "if end_session_when_done is True: end_session()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XNY6Zxl2Hfs2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}